Review and critique the discussion I've just had with Codex: I've been thinking about how best to simulate interview conversations with Alvia. I like the idea of being able to create and  
  simulate different personas based on insights about populations                                                                                                                             
    and how they might respond to Alvia's questions. You know, the whole digital twin kinda thing. Trouble is, I don't know how best to plug a digital respondent in to Alvia to take part in 
   the                                                                                                                                                                                        
    interview. Would that need to be an abstraction layer to allow a simulator to connect in? Or is there a more hacky way to make it work. I'd obviously need to build the virtual           
  interviewee,                                                                                                                                                                                
    which would be an interesting challenge in itself. Let's kick around some ideas.                                                                                                          
                                                                                                                                                                                              
                                                                                                                                                                                              
  • You want to design a “digital respondent” that can join Alvia interviews and behave like realistic personas, and you’re weighing a clean abstraction versus a hacky integration path. I’m 
   going                                                                                                                                                                                      
    to quickly inspect the current interview connection flow in the repo so we can discuss options that fit your actual architecture.                                                         
                                                                                                                                                                                              
  • Explored                                                                                                                                                                                  
    └ Search ws/interview|WebSocket|interview in voice-interview.ts                                                                                                                           
      List voice-interview                                                                                                                                                                    
                                                                                                                                                                                              
  ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── 
  ───────                                                                                                                                                                                     
                                                                                                                                                                                              
  • I confirmed your current flow is browser client WebSocket -> server /ws/interview -> provider WebSocket, with the browser already able to send both audio and text events. Next I’ll      
  inspect the                                                                                                                                                                                 
    exact client message protocol so we can map a simulator onto it with minimal new code.                                                                                                    
                                                                                                                                                                                              
  • Explored                                                                                                                                                                                  
    └ Read voice-interview.ts, interview.tsx                                                                                                                                                  
                                                                                                                                                                                              
  ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── 
  ───────                                                                                                                                                                                     
                                                                                                                                                                                              
  • I found a key detail: the server already supports text_input over the same interview WebSocket, which means a digital respondent can participate without audio synthesis at all. I’ll     
  inspect                                                                                                                                                                                     
    the connection bootstrap and auth/session checks next, since that determines whether simulator should run in-browser, server-side, or as a separate bot client.                           
                                                                                                                                                                                              
  • Questions 1/1 answered                                                                                                                                                                    
    • Which goal matters most for your first version of digital twins in Alvia?                                                                                                               
      answer: Fast internal simulator (Recommended)                                                                                                                                           
                                                                                                                                                                                              
  • Questions 1/1 answered                                                                                                                                                                    
    • How should the digital respondent generate answers initially?                                                                                                                           
      answer: LLM + persona prompt (Recommended)                                                                                                                                              
                                                                                                                                                                                              
  • Explored                                                                                                                                                                                  
    └ Read interview-access.routes.ts, interview-flow.routes.ts, voice-interview.ts                                                                                                           
      Search clientWs.send\(|type: \"connected\"|type: \"user_transcript\"|type: \"ai_transcript\"|type:                                                                                      
  \"question_changed\"|prompt_additional_questions_consent|additional_question_started|                                                                                                       
  error\" in                                                                                                                                                                                  
             voice-interview.ts                                                                                                                                                               
      Read voice-interview.ts                                                                                                                                                                 
                                                                                                                                                                                              
  ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── 
  ───────                                                                                                                                                                                     
                                                                                                                                                                                              
  • Your current codebase already gives you a good “hacky-but-real” path, and it can evolve into a clean abstraction later.                                                                   
                                                                                                                                                                                              
    - server/voice-interview.ts already accepts text_input from the client WebSocket and drives full Alvia/Barbara flow.                                                                      
    - client/src/pages/interview.tsx shows the exact message protocol (audio_ready, text_input, next_question, AQ consent events, etc.).                                                      
    - server/routes/interview-flow.routes.ts already creates sessions you can attach to.                                                                                                      
                                                                                                                                                                                              
    Best path for your chosen goal (fast internal simulator + LLM persona):                                                                                                                   
                                                                                                                                                                                              
    1. Build a digital-respondent-runner script that behaves like the browser client.                                                                                                         
    2. Create a session via API, connect to /ws/interview?sessionId=....                                                                                                                      
    3. Send audio_ready, then loop:                                                                                                                                                           
    4. Wait for Alvia events (ai_transcript_done, question_changed).                                                                                                                          
    5. Generate persona answer with LLM from a persona card.                                                                                                                                  
    6. Send answer as text_input.                                                                                                                                                             
    7. Decide when to send next_question, and handle AQ events.                                                                                                                               
                                                                                                                                                                                              
    This is the “hacky” route, but it is not brittle because it uses your real production interview pipeline end-to-end.                                                                      
                                                                                                                                                                                              
    If/when you harden it, add a thin abstraction:                                                                                                                                            
                                                                                                                                                                                              
    - RespondentAgent (decides what to say)                                                                                                                                                   
    - RespondentTransport (WebSocket protocol adapter)                                                                                                                                        
    - PersonaModel (traits, memory, consistency rules)                                                                                                                                        
                                                                                                                                                                                              
    That lets you swap in deterministic rule bots, LLM twins, or even voice twins later without touching interview orchestration logic.                                                       
                                                                                                                                                                                              
    If you want, next I can draft a decision-complete spec for:                                                                                                                               
                                                                                                                                                                                              
    - persona schema,                                                                                                                                                                         
    - simulator event loop/state machine,                                                                                                                                                     
    - evaluation metrics (realism, consistency, coverage),                                                                                                                                    
    - and where to place files/modules in this repo.                                                                                                                                          

● I'll explore the key areas Codex referenced to validate its analysis and form my own assessment.

● 3 Explore agents finished (ctrl+o to expand)
   ├─ Explore WebSocket interview protocol · 30 tool uses · 104.0k tokens
   │  ⎿  Done
   ├─ Explore Barbara orchestrator coupling · 25 tool uses · 84.1k tokens
   │  ⎿  Done
   └─ Explore session creation and consent flow · 51 tool uses · 83.8k tokens
      ⎿  Done

● Excellent research is back. Let me now read a couple of critical sections myself to validate the key claims before writing my critique.

● Read 2 files (ctrl+o to expand)

● Now I have enough to write a thorough critique. Let me synthesize everything.

● Updated plan
  ⎿  /plan to preview

● Now let me ask the key clarifying question before finalizing:
                                                                                                                                                                                                  
● User answered Claude's questions:                                                                                                                                                               
  ⎿  · What's the primary goal for your digital twin simulations? This determines whether we need the full Realtime API pipeline or a much cheaper text-only approach. → Test question design
     · Do you want this as a feature within Alvia's UI (researchers launch simulations from the dashboard), or as a developer/CLI tool for internal testing? → In-product UI (Recommended)
                                                                                                                                                                                                  
● Good -- testing question design via in-product UI confirms that the text-only ChatCompletions approach (Option A) is the right path. The Realtime API pipeline would add cost and latency with
  no benefit for this use case.                                                                                                                                                                   
                                                                                                                                                                  
  Let me ask a couple more targeted questions to scope the implementation:                                                                                                                        
                                                                                                                                                                                                  
● User answered Claude's questions:                                                                                                                                                               
  ⎿  · How should simulated sessions relate to real ones in the data model? This affects how researchers view and compare results. → Flagged sessions in existing collections (Recommended)       
     · What scope feels right for the first version of persona configuration? → Structured persona cards (Recommended)                                                                            
     · Is this something you want to start building now, or is this more of a design exercise to have a solid plan ready for later? → Design spec only                                            
                                                                                                                                                                                                  
● Design spec only, with flagged sessions and structured persona cards. Let me design this properly.                                                                                              
                                                                                                                                                                                  
● Plan(Design simulation engine spec)                                                                                                                                                             
  ⎿  Done (43 tool uses · 107.3k tokens · 3m 57s)                                                                                                                                                 
  (ctrl+o to expand)                                                                                                                                                                              
                                                                                                                                                                                                  
● Excellent. Now let me consolidate everything into the final plan document -- the Codex critique plus the full design spec.                                                                      
                                                                                                                                                                                                  
● Read 1 file (ctrl+o to expand)                                                                                                                                                                  
                                                                                                                                                                                                  
● Now let me write the final consolidated spec that includes the critique, the architecture recommendation, and the full implementation design.                                                   
                                                                                                                                                                                                  
● Updated plan                                                                                                                                                                                    
  ⎿  /plan to preview                                                                                                                                                                             
                                                                                                                                                                                                  
● The plan is complete. Here's a summary of what the document covers:                                                                                                                             
                                                                                                                                                                                                  
  Part 1 -- Codex Critique: Codex correctly identified the text_input integration surface, Barbara's text-decoupling, and the event loop shape. But it critically missed that every text_input    
  still generates full audio via the Realtime API (~60-70x more expensive than ChatCompletions), ignored speed/parallelism bottlenecks, framed production pipeline coupling as a benefit (it's a  
  liability for simulation), and hand-waved the persona modeling challenge.
                                                                                                                                                                                                  
  Part 2 -- Design Spec: A complete alternative architecture using a text-only ChatCompletions pipeline that reuses Alvia's prompting logic (buildInterviewInstructions()) and all of Barbara's
  orchestration functions without touching the Realtime API. Covers schema changes (2 new tables, 4 new columns), simulation engine turn loop, persona prompt system, 10 API endpoints, 6 new UI
  components, analytics integration, LLM tracking, risks/mitigations, and a 5-phase implementation order.

──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
 Ready to code?                                       

 Here is Claude's plan:                                                                                                                                                                           
╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌
 Digital Respondent Simulation: Codex Critique + Design Spec                                                                                                                                      
                                                        
 Part 1: Critique of the Codex Discussion

 Context

 You discussed building a "digital twin" simulation system with Codex -- virtual respondents that can participate in Alvia interviews using LLM-powered personas. Codex proposed a "hacky but
 real" WebSocket client approach that plugs into the existing interview pipeline via text_input.

 What Codex Got Right

 1. The text_input path exists and is well-suited. Confirmed at voice-interview.ts:2345-2428. Text input creates identical TranscriptEntry objects as audio STT, triggers Barbara identically,
 and persists the same way.
 2. The event loop sketch is accurate. Create session -> connect WS -> send audio_ready -> wait for ai_transcript_done -> send text_input -> handle question_changed -> handle AQ flow. Correct
 protocol.
 3. Barbara is fully text-decoupled. analyzeWithBarbara() takes BarbaraAnalysisInput which is purely transcript text. No audio dependency whatsoever.
 4. No auth required on the interview pipeline. Session creation, data fetch, and WebSocket are all public endpoints.

 What Codex Got Wrong

 Critical Issue #1: "Without audio synthesis" is misleading. Every text_input still triggers response.create with output_modalities: ["audio"] (line 2422). The Realtime API generates full PCM16
  audio for every Alvia response. A text-only respondent forces Alvia to generate audio that nobody listens to. The Realtime API IS Alvia's brain -- there's no bypass.

 Critical Issue #2: Cost was completely ignored. Realtime API audio output is ~$40/M tokens vs ChatCompletions at ~$0.60/M. Running 50 simulations through the Realtime API means paying 60-70x
 more for audio tokens that get discarded. This is the elephant in the room.

 Critical Issue #3: Speed bottleneck. The Realtime API streams audio in real-time (~3-8s per Alvia turn). A 10-question interview takes 5-10 minutes. 50 simulations = 4-8 hours sequentially.
 ChatCompletions would do it in ~30 minutes.

 Issue #4: "Uses your production pipeline" framed as a benefit. For simulation, the audio machinery (VAD, transcription quality, silence detection, barge-in) is irrelevant noise that adds cost,
  latency, and failure modes.

 Issue #5: Persona modeling was hand-waved. "Generate persona answer with LLM from a persona card" skips the hardest problems: consistency across 30 turns, realistic imperfections (hesitation,
 off-topic responses, short answers), and calibration against real interview data.

 Verdict

 Codex correctly identified the integration surface but proposed the wrong architecture. The right approach is a text-only ChatCompletions pipeline that reuses Alvia's prompting logic and
 Barbara's orchestration without touching the Realtime API.

 ---
 Part 2: Design Spec -- Simulation System

 Architecture Overview

 SimulationRunner (server/simulation/engine.ts)
   |
   |-- Alvia Adapter (ChatCompletions, reuses buildInterviewInstructions())
   |-- Persona Engine (ChatCompletions, structured persona cards)
   |-- Barbara Integration (reuse existing analyzeWithBarbara(), summaries, AQs)
   |-- Storage (reuse existing DatabaseStorage for sessions, segments, respondents)
   |
   |-- New: personas table, simulation_runs table
   |-- New: isSimulated flag on sessions
   |-- New: UI for persona management + simulation launch

 1. Database Schema Changes

 New table: personas (project-level, reusable across templates)

 personas
   id              varchar PK
   projectId       varchar FK -> projects (CASCADE)
   name            text NOT NULL          -- "Skeptical Power User"
   description     text                   -- Summary for researcher
   ageRange        text                   -- "25-34"
   gender          text
   occupation      text
   location        text
   attitude        enum(cooperative, reluctant, neutral, evasive, enthusiastic)
   verbosity       enum(low, medium, high)
   domainKnowledge enum(none, basic, intermediate, expert)
   traits          text[]                 -- ["analytical", "impatient"]
   communicationStyle  text               -- "Uses jargon, goes off-topic"
   backgroundStory text                   -- Optional narrative
   topicsToAvoid   text[]                 -- Subjects they deflect
   biases          text[]                 -- "prefers open-source"
   isArchived      boolean DEFAULT false
   createdAt       timestamp
   updatedAt       timestamp

 New table: simulation_runs (tracks batches)

 simulation_runs
   id                    varchar PK
   collectionId          varchar FK -> collections (CASCADE)
   launchedBy            varchar NOT NULL    -- userId
   status                enum(pending, running, completed, failed, cancelled)
   personaIds            text[] NOT NULL
   enableBarbara         boolean DEFAULT true
   enableSummaries       boolean DEFAULT true
   enableAdditionalQuestions  boolean DEFAULT true
   totalSimulations      integer NOT NULL
   completedSimulations  integer DEFAULT 0
   failedSimulations     integer DEFAULT 0
   errorMessage          text
   startedAt             timestamp
   completedAt           timestamp
   createdAt             timestamp

 Altered columns:

 - interviewSessions: add isSimulated (boolean, default false), personaId (FK -> personas), simulationRunId (FK -> simulation_runs)
 - respondents: add isSimulated (boolean, default false)

 2. Simulation Engine

 File: server/simulation/engine.ts (~450 lines)

 Core turn loop for each persona:

 1. Create respondent (isSimulated=true) + session (isSimulated=true, personaId, simulationRunId)
 2. Load template, questions, project context
 3. For each question (0..N):
    a. Build Alvia prompt via buildInterviewInstructions() + text-mode adapter
    b. Call ChatCompletions -> Alvia's opening message for this question
    c. Add to transcript
    d. FOLLOW-UP LOOP (max: recommendedFollowUps + 2, hard cap 8):
       i.   Build persona prompt with full conversation history
       ii.  Call ChatCompletions -> persona response
       iii. Add to transcript, update metrics (wordCount, turnCount)
       iv.  If Barbara enabled: call analyzeWithBarbara()
       v.   If Barbara says "suggest_next_question" -> break
       vi.  Build Alvia prompt with Barbara guidance
       vii. Call ChatCompletions -> Alvia follow-up
       viii.Add to transcript
    e. Call generateQuestionSummary() (existing Barbara function)
    f. Save segment via storage.createSegment()
 4. If AQs enabled: call generateAdditionalQuestions()
 5. For each AQ: repeat follow-up loop
 6. Generate session summaries (Alvia via ChatCompletions, Barbara via existing function)
 7. Mark session completed, update simulation run progress

 Termination safeguards:
 - Barbara's suggest_next_question action (primary signal)
 - Hard cap: recommendedFollowUps + 2 turns per question
 - Global fallback: max 8 turns per question
 - Per-question timeout: 5 minutes; per-session timeout: 30 minutes

 3. Alvia Text-Mode Adapter

 File: server/simulation/alvia-adapter.ts (~180 lines)

 - Calls buildInterviewInstructions() from server/voice-interview/instructions.ts directly
 - Appends text-mode override: "This is a text-based simulation. Do not mention buttons, clicking, audio, or speaking aloud."
 - Calls OpenAI ChatCompletions (model configurable, default gpt-4o-mini)
 - Tracks usage via withTrackedLlmCall() with use case simulation_alvia

 4. Persona Prompt Builder

 File: server/simulation/persona-prompt.ts (~200 lines)

 Builds system prompt from persona card:
 You are role-playing as a research interview respondent. Stay in character.

 PERSONA: [name, age, gender, occupation, location]
 ATTITUDE: [cooperative/reluctant/etc] -- behavioral description
 VERBOSITY: [low=1-2 sentences, medium=3-5, high=5+ detailed]
 DOMAIN KNOWLEDGE: [none/basic/intermediate/expert]
 TRAITS: [comma-separated]
 COMMUNICATION STYLE: [free text]
 BACKGROUND: [story]
 AVOID TOPICS: [list]
 BIASES: [list]

 BEHAVIORAL RULES:
 1. Match verbosity level strictly
 2. If reluctant, give short/deflecting answers sometimes
 3. Stay consistent with background and prior answers
 4. Never break character

 - Uses ChatCompletions with temperature based on verbosity (low=0.6, medium=0.8, high=1.0)
 - Tracks usage via withTrackedLlmCall() with use case simulation_persona

 5. Question Flow Logic

 File: server/simulation/question-flow.ts (~220 lines)

 - Extracts question progression from voice-interview.ts without WebSocket coupling
 - Handles conditional logic evaluation
 - Manages AQ phase entry/progression
 - Returns { action: "continue" | "next_question" | "start_aq" | "complete" }

 6. API Endpoints

 Persona CRUD (server/routes/persona.routes.ts, ~200 lines):

 ┌────────┬───────────────────────────────────┬───────────────────────┐
 │ Method │             Endpoint              │      Description      │
 ├────────┼───────────────────────────────────┼───────────────────────┤
 │ GET    │ /api/projects/:projectId/personas │ List personas         │
 ├────────┼───────────────────────────────────┼───────────────────────┤
 │ GET    │ /api/personas/:id                 │ Get persona           │
 ├────────┼───────────────────────────────────┼───────────────────────┤
 │ POST   │ /api/projects/:projectId/personas │ Create persona        │
 ├────────┼───────────────────────────────────┼───────────────────────┤
 │ PATCH  │ /api/personas/:id                 │ Update persona        │
 ├────────┼───────────────────────────────────┼───────────────────────┤
 │ DELETE │ /api/personas/:id                 │ Archive (soft delete) │
 └────────┴───────────────────────────────────┴───────────────────────┘

 Simulation control (server/routes/simulation.routes.ts, ~350 lines):

 ┌────────┬────────────────────────────────────────────────┬──────────────────────────────┐
 │ Method │                    Endpoint                    │         Description          │
 ├────────┼────────────────────────────────────────────────┼──────────────────────────────┤
 │ POST   │ /api/collections/:collectionId/simulate        │ Launch run (max 10 personas) │
 ├────────┼────────────────────────────────────────────────┼──────────────────────────────┤
 │ GET    │ /api/collections/:collectionId/simulation-runs │ List runs                    │
 ├────────┼────────────────────────────────────────────────┼──────────────────────────────┤
 │ GET    │ /api/simulation-runs/:id                       │ Get status + progress        │
 ├────────┼────────────────────────────────────────────────┼──────────────────────────────┤
 │ POST   │ /api/simulation-runs/:id/cancel                │ Cancel running simulation    │
 └────────┴────────────────────────────────────────────────┴──────────────────────────────┘

 Launch runs asynchronously -- HTTP returns immediately with run ID, engine processes in background.

 7. UI Changes

 New components:

 ┌────────────────────────┬────────────────────────────────────────────────────────────────────┐
 │       Component        │                              Purpose                               │
 ├────────────────────────┼────────────────────────────────────────────────────────────────────┤
 │ PersonaManager.tsx     │ CRUD list for persona cards within project detail                  │
 ├────────────────────────┼────────────────────────────────────────────────────────────────────┤
 │ PersonaCard.tsx        │ Display card for a single persona                                  │
 ├────────────────────────┼────────────────────────────────────────────────────────────────────┤
 │ PersonaFormDialog.tsx  │ Create/edit form (demographics, attitude, verbosity, traits, etc.) │
 ├────────────────────────┼────────────────────────────────────────────────────────────────────┤
 │ SimulationLauncher.tsx │ Launch dialog: select personas, configure options                  │
 ├────────────────────────┼────────────────────────────────────────────────────────────────────┤
 │ SimulationProgress.tsx │ Progress indicator for running simulations                         │
 ├────────────────────────┼────────────────────────────────────────────────────────────────────┤
 │ SimulationBadge.tsx    │ Visual badge marking sessions as simulated                         │
 └────────────────────────┴────────────────────────────────────────────────────────────────────┘

 Page changes:

 - project-detail.tsx: Add "Personas" tab with PersonaManager
 - collection-detail.tsx: Add "Simulate" button in header -> SimulationLauncher dialog; show SimulationProgress for active runs; add SimulationBadge on simulated sessions; add filter toggle for
  simulated sessions
 - session-detail.tsx: Show SimulationBadge + persona info card for simulated sessions
 - sessions.tsx: Add simulated/real filter; show badge on list items

 New hook: use-simulation-progress.ts -- polls run status every 3s while running.

 8. Analytics Integration

 - Analytics refresh endpoints accept includeSimulated query param (default: false)
 - Collection analytics UI gets "Include simulated data" toggle
 - Cross-interview context for real interviews always excludes simulated sessions
 - Dashboard stats show "12 completed (3 simulated)" breakdown

 9. LLM Usage Tracking

 Add 2 new use cases to shared/types/llm-usage.ts:
 - simulation_alvia -- Alvia's ChatCompletions responses
 - simulation_persona -- Persona's ChatCompletions responses

 Barbara calls during simulation use existing use cases (barbara_analysis, barbara_question_summary, etc.) -- attribution includes the simulated session ID.

 10. Storage Changes

 New methods in server/storage/simulation.ts (composition, not growing storage.ts):
 - getPersona, getPersonasByProject, createPersona, updatePersona, deletePersona
 - getSimulationRun, getSimulationRunsByCollection, createSimulationRun, updateSimulationRun

 11. Key Risks and Mitigations

 ┌─────────────────────────────────────────────────────────┬─────────────────────────────────────────────────────────────────────────────────────┐
 │                          Risk                           │                                     Mitigation                                      │
 ├─────────────────────────────────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
 │ Alvia prompt has voice-specific language ("click Next") │ Append text-mode override. Do NOT fork buildInterviewInstructions().                │
 ├─────────────────────────────────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
 │ Turn loop cycles indefinitely                           │ Barbara + hard caps + timeouts (8 turns, 5min/question, 30min/session)              │
 ├─────────────────────────────────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
 │ Rate limiting on ChatCompletions                        │ Max 2 concurrent simulations, 200ms inter-turn delay, max 10 personas/run           │
 ├─────────────────────────────────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
 │ storage.ts growth (1630 lines, must only shrink)        │ New methods in server/storage/simulation.ts via composition                         │
 ├─────────────────────────────────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
 │ Simulated data contaminates real analytics              │ isSimulated default false; analytics excludes by default; prominent badges          │
 ├─────────────────────────────────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
 │ Persona responses too uniform/"helpful AI"              │ Strong behavioral prompts, variable temperature, concrete examples in system prompt │
 └─────────────────────────────────────────────────────────┴─────────────────────────────────────────────────────────────────────────────────────┘

 12. Implementation Order

 Phase 1 -- Foundation: Schema changes, persona CRUD (API + UI), db:push
 Phase 2 -- Engine: Simulation types, persona prompt builder, Alvia adapter, question flow, core turn loop, LLM tracking
 Phase 3 -- Launch UI: Simulation routes, launcher dialog, progress component, hook
 Phase 4 -- Visual Integration: SimulationBadge on all session views, filters
 Phase 5 -- Analytics: includeSimulated filtering, toggle in analytics UI, cross-interview context exclusion

 13. Key Files to Modify/Create

 Create:
 - server/simulation/engine.ts -- Core turn loop (~450 lines)
 - server/simulation/alvia-adapter.ts -- Alvia ChatCompletions adapter (~180 lines)
 - server/simulation/persona-prompt.ts -- Persona prompt builder (~200 lines)
 - server/simulation/question-flow.ts -- Question progression (~220 lines)
 - server/simulation/types.ts -- Type definitions (~100 lines)
 - server/routes/persona.routes.ts -- Persona API (~200 lines)
 - server/routes/simulation.routes.ts -- Simulation API (~350 lines)
 - server/storage/simulation.ts -- Storage methods (~150 lines)
 - shared/types/simulation.ts -- Shared types (~80 lines)
 - 6 new React components (PersonaManager, PersonaCard, PersonaFormDialog, SimulationLauncher, SimulationProgress, SimulationBadge)
 - client/src/hooks/use-simulation-progress.ts

 Modify:
 - shared/schema.ts -- Add personas, simulation_runs tables; add columns to interviewSessions, respondents
 - shared/types/llm-usage.ts -- Add 2 new use cases
 - server/routes/index.ts -- Register new route modules
 - server/routes/analytics.routes.ts -- Add includeSimulated filtering
 - client/src/pages/project-detail.tsx -- Add Personas tab
 - client/src/pages/collection-detail.tsx -- Add Simulate button, progress, badges, filter
 - client/src/pages/session-detail.tsx -- Add persona info for simulated sessions
 - client/src/pages/sessions.tsx -- Add simulated filter/badge

 Reuse directly (no changes):
 - server/voice-interview/instructions.ts -- buildInterviewInstructions()
 - server/barbara-orchestrator.ts -- analyzeWithBarbara(), generateQuestionSummary(), generateAdditionalQuestions(), generateSessionSummary()
 - server/voice-interview/context-builders.ts -- Cross-interview context
 - server/voice-interview/transcript.ts -- addTranscriptEntry()
 - server/llm-usage.ts -- withTrackedLlmCall()

 Verification

 To test end-to-end:
 1. Create a persona via the project detail Personas tab
 2. Launch a simulation from a collection with 1 persona
 3. Verify simulation run status progresses (pending -> running -> completed)
 4. Check the simulated session appears in collection detail with SimulationBadge
 5. Open the simulated session -- verify transcript shows natural Alvia/persona conversation
 6. Verify Barbara guidance was applied (check lastBarbaraGuidance on session)
 7. Verify question summaries and session summary were generated
 8. Verify segments were created for each question
 9. Refresh collection analytics with includeSimulated=false -- simulated session excluded
 10. Refresh with includeSimulated=true -- simulated session included
 11. Check LLM usage tracking shows simulation_alvia and simulation_persona use cases